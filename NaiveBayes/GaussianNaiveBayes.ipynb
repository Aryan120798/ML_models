{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from dataclasses import dataclass\n",
    "import sklearn\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make_blobs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_blobs(n_samples = 10000, n_features = 2, centers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.74905974 -3.98765136]\n",
      " [ 9.27196757 -5.13814732]\n",
      " [ 6.30101368 -4.23083817]\n",
      " [10.44921614 -6.65233778]\n",
      " [10.49850066 -5.74158534]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points about NB:\n",
    "- Generative model\n",
    "- Non-parametric model\n",
    "- It assumes a gaussian distribution for every column of each class\n",
    "- It naively assumes that columns are independent\n",
    "  \n",
    "\n",
    "- It doesn't have any hyper-parameters\n",
    "\n",
    "What do we need:\n",
    "- Prior Distribution\n",
    "- Likelihood\n",
    "- Posterior ~ Likelihood * Prior\n",
    "- Datasplit\n",
    "\n",
    "Gaussian_PDF = 1/(sqrt(2 * pi * var) * (exp(-(x_i - mean)**2/2var))\n",
    "p(y_i = k | x_i) ~ P(x_i_0 | y = k) * P(x_i_1 | y = k) * P(y=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GaussianNaiveBayes:\n",
    "    # def __init__(self, X, y):\n",
    "    #     self.X = X\n",
    "    #     self.y = y\n",
    "    X: np.array\n",
    "    y: np.array\n",
    "\n",
    "    def __post_init__(self): #series of steps that you want your class object to execute/ initiate as soon as someone initiates the class without a need to ask the user\n",
    "        self.dataSplit()\n",
    "        \n",
    "        self.X0_train = self.X_train[self.y_train == 0]\n",
    "        self.X1_train = self.X_train[self.y_train == 1]\n",
    "        self.y0_train = self.y_train[self.y_train == 0]\n",
    "        self.y1_train = self.y_train[self.y_train == 1]\n",
    "\n",
    "        self.fit()\n",
    "        self.predict()\n",
    "    #a new thing taught about data class\n",
    "\n",
    "    def dataSplit(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = sklearn.model_selection.train_test_split(self.X, self.y, test_size = 0.3, shuffle = True)\n",
    "\n",
    "    def fit_distribution(self, x): #we are passing one column at a time\n",
    "        mean = np.mean(x)\n",
    "        std = np.std(x)\n",
    "        dist = norm(mean, std)\n",
    "        return dist \n",
    "\n",
    "    def posterior(self, x, prior, dist_col1, dist_col2):\n",
    "        return prior * dist_col1.pdf(x[0]) * dist_col2.pdf(x[1]) # dist_col1.pdf(x[0]) is the likelihood of that point in that distribution\n",
    "        \n",
    "    def fit(self):\n",
    "        self.prior_y0 = len(self.y0_train) / len(self.y)\n",
    "        self.prior_y1 = len(self.y1_train) / len(self.y)\n",
    "\n",
    "        #Distribution of every column in each class\n",
    "        \n",
    "        self.dist_X_0_0 = self.fit_distribution(self.X0_train[:,0])\n",
    "        self.dist_X_0_1 = self.fit_distribution(self.X0_train[:,1])\n",
    "\n",
    "        self.dist_X_1_0 = self.fit_distribution(self.X1_train[:,0])\n",
    "        self.dist_X_1_1 = self.fit_distribution(self.X1_train[:,1])\n",
    "\n",
    "    def predict(self):\n",
    "\n",
    "        self.error_count = 0\n",
    "\n",
    "        for sample, target in zip(self.X_test, self.y_test):\n",
    "            py0 = self.posterior(sample, self.prior_y0, self.dist_X_0_0, self.dist_X_0_1)\n",
    "            py1 = self.posterior(sample, self.prior_y1, self.dist_X_1_0, self.dist_X_1_1)\n",
    "\n",
    "            #print('P(y=0 | %s) = %3.f' % (sample, py0 * 100))\n",
    "            #print('P(y=1 | %s) = %3.f' % (sample, py1 * 100))\n",
    "\n",
    "            if np.argmax([py0,py1]) != target:\n",
    "                self.error_count += 1\n",
    "        \n",
    "            #print(\" Model predicted class {} and the truth was: {} \\n\".format(np.argmax([py0,py1]), target))\n",
    "\n",
    "        print(self.error_count)\n",
    "        print(len(self.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNaiveBayes(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet39",
   "language": "python",
   "name": "prophet39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
